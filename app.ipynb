{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Install necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q transformers\n",
    "!pip install xformers\n",
    "!pip install -q datasets\n",
    "!pip install -q trl\n",
    "!pip install git+https://github.com/huggingface/peft.git\n",
    "!pip install -q bitsandbytes==0.37.2\n",
    "!pip install -q -U accelerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "import bitsandbytes as bnb\n",
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training, PeftModel, PeftConfig\n",
    "from datasets import load_dataset\n",
    "from transformers import TrainingArguments, pipeline\n",
    "from trl import SFTTrainer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load a model and tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# login to huggingface\n",
    "from huggingface_hub import login\n",
    "login()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "repo_id = \"meta-llama/Llama-2-7b-chat-hf\" # Modify to whatever model you want to use\n",
    "\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    repo_id,\n",
    "    device_map='auto',\n",
    "    load_in_8bit=True,\n",
    "    trust_remote_code=True,\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(repo_id)\n",
    "tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "base_model.config.use_cache = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(base_model) # use it to check what target module should be"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model.get_memory_footprint() # Check the memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to see how many parameters the model has:\n",
    "def print_trainable_parameters(model):\n",
    "    \"\"\"\n",
    "    Prints the number of trainable parameters in the model.\n",
    "    \"\"\"\n",
    "    trainable_params = 0\n",
    "    all_param = 0\n",
    "    for _, param in model.named_parameters():\n",
    "        all_param += param.numel()\n",
    "        if param.requires_grad:\n",
    "            trainable_params += param.numel()\n",
    "    print(\n",
    "        f\"trainable params: {trainable_params} || all params: {all_param} || trainable%: {100 * trainable_params / all_param:.2f}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test the base model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda:0\"\n",
    "\n",
    "def user_prompt(human_prompt):\n",
    "    prompt_template=f\"### HUMAN:\\n{human_prompt}\\n\\n### RESPONSE:\\n\" # This has to change if your dataset isn't formatted as Alpaca\n",
    "    return prompt_template\n",
    "\n",
    "pipe = pipeline(\n",
    "    task=\"text-generation\",\n",
    "    model=base_model,\n",
    "    tokenizer=tokenizer,\n",
    "    max_length=150,\n",
    "    repetition_penalty=1.15,\n",
    "    top_p=0.95\n",
    "    )\n",
    "result = pipe(user_prompt(\"You are an expert youtuber. Give me some ideas for a youtube title for a video about fine tuning LLM\"))\n",
    "print(result[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare and preprocess the model for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = LoraConfig(\n",
    "    r=8,\n",
    "    lora_alpha=32,\n",
    "    target_modules=[\"q_proj\", \"v_proj\", \"k_proj\", \"o_proj\"], # you have to know the target modules, it varies from model to model\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\"\n",
    ")\n",
    "\n",
    "\n",
    "model = get_peft_model(base_model, config) # Wrap the base model with get_peft_model() to get a trainable PeftModel\n",
    "print_trainable_parameters(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load a dataset from datasets library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"csv\", data_files = \"you_data_here.csv\") # substitute with whatever file name you have\n",
    "print(\"Dataset loaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adam_bits = 8\n",
    "\n",
    "training_arguments = TrainingArguments(\n",
    "    output_dir = \"Trainer_output\",\n",
    "    per_device_train_batch_size = 1,\n",
    "    gradient_accumulation_steps = 4,\n",
    "    run_name=f\"deb-v2-xl-{adam_bits}bitAdam\",\n",
    "    logging_steps = 20,\n",
    "    learning_rate = 2e-4,\n",
    "    fp16=True,\n",
    "    max_grad_norm = 0.3,\n",
    "    max_steps = 300,\n",
    "    warmup_ratio = 0.03,\n",
    "    group_by_length=True,\n",
    "    lr_scheduler_type = \"constant\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = SFTTrainer(\n",
    "    model = model,\n",
    "    train_dataset = dataset[\"train\"],\n",
    "    dataset_text_field=\"text\",\n",
    "    args = training_arguments,\n",
    "    max_seq_length = 512,\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save the adapter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.save_model(\"Finetuned_adapter\")\n",
    "adapter_model = model\n",
    "\n",
    "print(\"Lora Adapter saved\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Merge the base model and the adapter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Can't merge the 8 bit/4 bit model with Lora so reload it\n",
    "\n",
    "repo_id = \"meta-llama/Llama-2-7b-chat-hf\"\n",
    "use_ram_optimized_load=False\n",
    "\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    repo_id,\n",
    "    device_map='auto',\n",
    "    trust_remote_code=True,\n",
    ")\n",
    "\n",
    "base_model.config.use_cache = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model.get_memory_footprint()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Lora adapter\n",
    "model = PeftModel.from_pretrained(\n",
    "    base_model,\n",
    "    \"/content/Finetuned_adapter\",\n",
    "    )\n",
    "merged_model = model.merge_and_unload()\n",
    "\n",
    "merged_model.save_pretrained(\"/content/Merged_model\")\n",
    "tokenizer.save_pretrained(\"/content/Merged_model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda:0\"\n",
    "\n",
    "def user_prompt(human_prompt):\n",
    "    prompt_template=f\"### HUMAN:\\n{human_prompt}\\n\\n### RESPONSE:\\n\"\n",
    "    return prompt_template\n",
    "\n",
    "pipe = pipeline(\n",
    "    task=\"text-generation\",\n",
    "    model=merged_model,\n",
    "    tokenizer=tokenizer,\n",
    "    max_length=150,\n",
    "    repetition_penalty=1.15,\n",
    "    top_p=0.95\n",
    "    )\n",
    "result = pipe(user_prompt(\"You are an expert youtuber. Give me some ideas for a youtube title for a video about fine tuning LLM\"))\n",
    "print(result[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_model.push_to_hub(\"your_hg_id/name_fine_tuned_model\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
